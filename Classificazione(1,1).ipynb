{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICAZIONE PURA\n",
    "\n",
    "Lo scopo di questo lavoro Ã¨ quello di classificare alcune recensioni Amazon attraverso metodi tradizionali di machine learning. \n",
    "In particolare, le rappresentazioni utilizzate sono BOW con pesi binari, frequenze e tf-idf ed un vocabolario costituito solo da 1-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3buo9VLXnzU5",
    "outputId": "5871c679-fe4c-4c18-c718-9b5037122472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6PoucXYhfOB",
    "outputId": "234a88a4-a6ac-43e2-e47c-e0dca1cde0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 27.4 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4n_hfnkwn3za",
    "outputId": "e085c908-fe9d-4872-a1fd-fba56d3456cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Requirement already satisfied: demoji in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from demoji) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.12 seconds)\n",
      "Writing emoji data to /root/.demoji/codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "# Plot.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP pipeline.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words = stop_words + ['would'] + ['-PRON-'] # -PRON- viene aggiunto dal lemmatizer e va tolto.\n",
    "import spacy # lemmatization\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "!pip install demoji\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "# Rappresentazione.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Classificatori.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cLI1dRKMCJ2W"
   },
   "outputs": [],
   "source": [
    "# Codifica le sigle nei rispettivi classificatori. \n",
    "def decode(c):\n",
    "  decoder = {'KNN': 'KNN', \n",
    "             'LSVC': 'Linear SVC', \n",
    "             'Logistic': 'Logistic', \n",
    "             'GNB': 'Gaussian Naive Bayes', \n",
    "             'Tree': 'Decision Tree Classifier', \n",
    "             'RF': 'Random Forest',\n",
    "             'SGD':'Stochastic Gradient Descent Classifier',\n",
    "             'MNB': 'Multinomial Naive Bayes', \n",
    "             'ADAB': 'Adaboost',\n",
    "             'BNB': 'Bernullian Naive Bayes'}\n",
    "  return decoder[c] \n",
    "\n",
    "\n",
    "# NLP pipeline.\n",
    "\n",
    "def pipe(doc):\n",
    "  doc = doc.lower()   # Lower case. \n",
    "  doc = re.sub(r'\\d+', ' ', doc)  # Drop digits.\n",
    "  doc = re.sub('['+string.punctuation+']', ' ', doc) # Drop punctuation. \n",
    "  doc = re.sub(r'\\n+', ' ', doc) # Drop newline.\n",
    "  if 'www.' in doc or 'http:' in doc or 'https:' in doc or '.com' in doc: # Drop URL.\n",
    "    doc = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \" \", doc)\n",
    "  doc = demoji.replace(string = doc, repl = \" \")  # Drop emoji.\n",
    "  doc = re.sub(r'\\s\\s+', ' ', doc) # Drop extra spaces. \n",
    "  doc = nlp(doc)  \n",
    "  doc = [token.lemma_ for token in doc]  # Tokenize and Lemmatization. \n",
    "  doc = [word for word in doc if len(word) > 2] # Drop short words.\n",
    "  doc = [word for word in doc if word not in stop_words] # Drop stop words.\n",
    "  return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NUGiH6Ch1w_0"
   },
   "outputs": [],
   "source": [
    "# NLP pipeline: load, sample, clean dataset.\n",
    "class preprocessing:\n",
    "  # Initialize.\n",
    "  def __init__(self, path, name_df):\n",
    "    self.path = path\n",
    "    self.name_df = name_df\n",
    "\n",
    "  # Load data.\n",
    "  def loader(self):\n",
    "    print('IMPORT DATASET ' + self.name_df)\n",
    "    if os.path.isfile(self.path) == False: \n",
    "      print('Set not exists.')\n",
    "      raise SystemExit(\"Stop right there!\")\n",
    "    else:\n",
    "      with open(self.path, encoding=\"utf8\") as file:\n",
    "       self.df = file.readlines()\n",
    "    print(f\"Size: {len(self.df)}\")\n",
    "\n",
    " # Sampling.\n",
    "  def sampling(self, size):\n",
    "    print('SAMPLING ' + self.name_df)\n",
    "    random.seed = 20201230\n",
    "    self.df = random.sample(self.df, size)  \n",
    "    print(f\"Size: {len(self.df)}\")\n",
    "\n",
    "  # From list to data frame\n",
    "  def data_frame(self):\n",
    "    print('CREATE DATASET: REVIEWS - LABELS ' + self.name_df)\n",
    "    X = []\n",
    "    labels = []\n",
    "\n",
    "    for rev in self.df:\n",
    "      _, label, sent = re.split(r'__label__(\\d)', rev)\n",
    "      label = int(label[0]) - 1\n",
    "      labels.append(label)\n",
    "      X.append(sent)\n",
    "\n",
    "    self.df = pd.DataFrame(list(zip(X, labels)), columns=['Review', 'Labels']) \n",
    "\n",
    "  # NLP pipeline.\n",
    "  def pipeline(self):\n",
    "    print('NLP pipeline ' + self.name_df)\n",
    "    t1 = time()\n",
    "    self.df[\"Clean_Review\"] = self.df[\"Review\"].apply(pipe)\n",
    "    t2 = time()\n",
    "    print(f\"Time: {t2-t1}\")\n",
    "\n",
    "  # Empty review,\n",
    "  def drop_empty_review(self):\n",
    "    print('DROP EMPTY REVIEW ' + self.name_df)\n",
    "    self.df.drop(self.df[self.df['Clean_Review'].map(len) == 0].index)\n",
    "\n",
    "# Set of possible classification techniques.\n",
    "class classification:\n",
    "    # Initialize.\n",
    "    def __init__(self, X_train, y_train, cv):\n",
    "        self.seed = 20201230\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.cv = cv\n",
    "\n",
    "    # Select the model.\n",
    "    def select_model(self, class_type):\n",
    "      possible_class = ['KNN', 'LSVC', 'Logistic', 'Tree', 'RF', 'SGD', 'MNB', 'ADAB', 'BNB']\n",
    "\n",
    "      if class_type in possible_class:\n",
    "        if class_type == possible_class[0]:\n",
    "          clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "        if class_type == possible_class[1]:\n",
    "          clf = LinearSVC(random_state=0, max_iter=1000)\n",
    "        if class_type == possible_class[2]:\n",
    "          clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "        if class_type == possible_class[3]:\n",
    "          clf = DecisionTreeClassifier()\n",
    "        if class_type == possible_class[4]:\n",
    "          clf = RandomForestClassifier()\n",
    "        if class_type == possible_class[5]:\n",
    "          clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "        if class_type == possible_class[6]:\n",
    "          clf = MultinomialNB()\n",
    "        if class_type == possible_class[7]:\n",
    "          clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "        if class_type == possible_class[8]:\n",
    "          clf = BernoulliNB()\n",
    "\n",
    "        self.clf = clf\n",
    "        # Cross validation.\n",
    "        self.score = cross_validate(self.clf, self.X_train, self.y_train, cv=self.cv, scoring = ['accuracy', 'f1'])\n",
    "        self.N = int(self.X_train.shape[0]/cv)\n",
    "        # Performance dell'algoritmo.\n",
    "        perf = []\n",
    "        for key in self.score:          \n",
    "          perf.append([key, self.score[key].mean(), self.score[key].std() * 2/np.sqrt(self.N)])\n",
    "\n",
    "        perf = np.array(perf)\n",
    "        perf =  pd.DataFrame(perf, columns = ['Measure', 'Mean', 'StandError'])\n",
    "        perf['Measure'] = ['TrainTime', 'TestTime', 'F1', 'Accuracy']\n",
    "        print(perf)\n",
    "        self.perf = perf\n",
    "      else:\n",
    "        raise SystemExit(\"Select one of the provided classifiers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIKI0kBVx960"
   },
   "source": [
    "### ***IMPORT AND PREPROCESSING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhtnq7HBBskd",
    "outputId": "ace4dd54-1705-4272-fdc4-b92122b98e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORT DATASET TRAIN\n",
      "Size: 3600000\n",
      "SAMPLING TRAIN\n",
      "Size: 250000\n",
      "CREATE DATASET: REVIEWS - LABELS TRAIN\n",
      "NLP pipeline TRAIN\n",
      "Time: 2074.779326438904\n",
      "DROP EMPTY REVIEW TRAIN\n",
      "CPU times: user 34min 32s, sys: 8.47 s, total: 34min 40s\n",
      "Wall time: 34min 46s\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "\n",
    "        #-- IMPORT SET.\n",
    "\n",
    "train_path = 'drive/MyDrive/Text Mining/train.ft.txt'\n",
    "\n",
    "train = preprocessing(train_path, 'TRAIN')\n",
    "\n",
    "train.loader()\n",
    "\n",
    "      #-- SAMPLING.\n",
    "\n",
    "train_sample_size = 250000\n",
    "train.sampling(train_sample_size)\n",
    "\n",
    "      #-- CREATE A DATASET WITH REVIEW - LABEL.\n",
    "\n",
    "train.data_frame()\n",
    "\n",
    "      #-- PREPROCESSING.\n",
    "\n",
    "train.pipeline()\n",
    "\n",
    "      #-- DROP EMPTY REVIEWS.\n",
    "\n",
    "train.drop_empty_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IElCkQuPyDFR"
   },
   "source": [
    "### ***FEATURE EXTRACTION - SELECTION - WEIGHTED***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QuhIXoqsw4g4"
   },
   "outputs": [],
   "source": [
    "# Number of min and max gram.\n",
    "gram_min = 1\n",
    "gram_max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMRUkukz8zyJ"
   },
   "source": [
    "#### ***BINARY MATRIX***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpMlzKqWzT-n",
    "outputId": "81ddf28a-c705-44b5-bfa2-02232996c005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 14138)\n",
      "(250000, 1414)\n",
      "CPU times: user 6.93 s, sys: 454 ms, total: 7.38 s\n",
      "Wall time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "    #-- FEATURE EXTRACTION: BINARY BOW.\n",
    "\n",
    "def dummy(doc):\n",
    "  return doc\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        min_df = 0.0001, # 0.0001*100000 = 10, ragionevole\n",
    "        # max_df = 0.8, # stop words removal: it's useless if I had removed stop words!\n",
    "        ngram_range = (gram_min, gram_max),\n",
    "        binary = True\n",
    "    )  \n",
    "\n",
    "train_BOW = vectorizer.fit_transform(train.df['Clean_Review'])\n",
    "\n",
    "print(train_BOW.shape)\n",
    "\n",
    "      #-- FEATURE SELECTION\n",
    "\n",
    "select_percentile = SelectPercentile(chi2, percentile = 10)\n",
    "train_BOW = select_percentile.fit_transform(train_BOW, train.df['Labels'])\n",
    "\n",
    "print(train_BOW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IUy3jFQyORt"
   },
   "source": [
    "###### ***CLASSIFICATION***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktRzl4gUn6Y-",
    "outputId": "28492b99-7dd4-4df2-cc93-515c59c4d9cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            ***** KNN ****\n",
      "     Measure                Mean              StandError\n",
      "0  TrainTime  0.0650641918182373   0.0001627359304443132\n",
      "1   TestTime   366.1159992694855     0.20861989063311057\n",
      "2         F1            0.707724  1.4579942386717673e-05\n",
      "3   Accuracy  0.7394337598322811   8.368565873105576e-06\n",
      "                                                            ***** Linear SVC ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime   45.843698263168335    0.003343354167321854\n",
      "1   TestTime  0.03264493942260742  3.4664502692565706e-06\n",
      "2         F1             0.874964  1.4487467687625725e-05\n",
      "3   Accuracy   0.8757086310613117  1.4083834320409967e-05\n",
      "                                                            ***** Logistic ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime     7.714233684539795   0.0015569618152876516\n",
      "1   TestTime  0.032081222534179686   4.588141428438142e-06\n",
      "2         F1    0.8748000000000001  1.3782307499109307e-05\n",
      "3   Accuracy    0.8753454333697899  1.3700625126108876e-05\n",
      "                                                            ***** Stochastic Gradient Descent Classifier ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime   0.9082088947296143   0.0003654670236986761\n",
      "1   TestTime  0.03200001716613769   5.371755862607002e-06\n",
      "2         F1   0.8723519999999999  1.6040987500774168e-05\n",
      "3   Accuracy   0.8729600514141342  1.8668056097315817e-05\n",
      "                                                            ***** Multinomial Naive Bayes ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime  0.07371954917907715  1.0437346632224037e-05\n",
      "1   TestTime  0.03485326766967774  2.4561646052482155e-06\n",
      "2         F1   0.8509360000000001  1.9387550644678935e-05\n",
      "3   Accuracy   0.8507749199665726   1.996425367210844e-05\n",
      "                                                            ***** Adaboost ****\n",
      "     Measure                Mean              StandError\n",
      "0  TrainTime  24.339518880844118    0.005633526751785389\n",
      "1   TestTime   1.193763256072998  0.00020275806748526915\n",
      "2         F1            0.821068   1.002052294044564e-05\n",
      "3   Accuracy  0.8209113847530023    9.56382877537668e-06\n",
      "                                                            ***** Bernullian Naive Bayes ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime  0.09976105690002442  1.2431459375341802e-05\n",
      "1   TestTime  0.04390878677368164   7.716557113115357e-06\n",
      "2         F1             0.845172  1.0177685395019798e-05\n",
      "3   Accuracy   0.8482745048531445  1.0533645365188172e-05\n",
      "CPU times: user 37min 1s, sys: 8.56 s, total: 37min 9s\n",
      "Wall time: 37min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = 5\n",
    "learn = classification(train_BOW, train.df['Labels'], cv)\n",
    "\n",
    "my_class = ['KNN', 'LSVC', 'Logistic', 'SGD', 'MNB', 'ADAB', 'BNB']\n",
    "\n",
    "for c in my_class:\n",
    "  print(f\"                                                            ***** {decode(c)} ****\")\n",
    "  learn.select_model(c)\n",
    "\n",
    "# del train_BOW, test_BOW, learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6195UAxLIiX7"
   },
   "source": [
    "#### ***FREQUENCY MATRIX***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XCRF-iDIh2C",
    "outputId": "c18e360a-2d82-41d2-ecfe-946aaaad8866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 14138)\n",
      "(250000, 1414)\n",
      "CPU times: user 6.99 s, sys: 192 ms, total: 7.18 s\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "    #-- FEATURE EXTRACTION: FREQUENCY BOW.\n",
    "    \n",
    "def dummy(doc):\n",
    "  return doc\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        min_df = 0.0001, # 0.001*100000 = 100, ragionevole\n",
    "        # max_df = 0.7,\n",
    "        ngram_range = (gram_min, gram_max),\n",
    "        binary = False, \n",
    "        # vocabulary = my_dic\n",
    "    )  \n",
    "\n",
    "# Restituisce un amatrice di TIPO sparso quindi non bisogna convertirla. \n",
    "train_freq = vectorizer.fit_transform(train.df['Clean_Review'])\n",
    "\n",
    "print(train_freq.shape)\n",
    "\n",
    "      #-- FEATURE SELECTION\n",
    "\n",
    "select_percentile = SelectPercentile(chi2, percentile=10)\n",
    "train_freq = select_percentile.fit_transform(train_freq, train.df['Labels'])\n",
    "\n",
    "print(train_freq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQNrZQ3ZsPod"
   },
   "source": [
    "##### ***CLASSIFICATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8fO1NU9r0Zr",
    "outputId": "aa18f865-35ab-4cc0-f4b5-5e5a5d18448e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            ***** KNN ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime  0.04533343315124512  3.8723125853056595e-05\n",
      "1   TestTime   362.12245497703555      0.1769799883243751\n",
      "2         F1             0.705504  1.8402225952313688e-05\n",
      "3   Accuracy   0.7344875823629196  1.5678424107273437e-05\n",
      "                                                            ***** Linear SVC ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Measure                  Mean              StandError\n",
      "0  TrainTime     50.42601866722107   0.0063329044665848015\n",
      "1   TestTime  0.032227706909179685   7.247391530546355e-06\n",
      "2         F1    0.8767760000000001  1.5295447688773445e-05\n",
      "3   Accuracy    0.8776897652176585    1.45783084658232e-05\n",
      "                                                            ***** Logistic ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime   10.180205821990967    0.003383445817106665\n",
      "1   TestTime  0.03125123977661133   4.465297953818192e-06\n",
      "2         F1   0.8766679999999999  1.5899927043857842e-05\n",
      "3   Accuracy   0.8772641482075352  1.5394059815732048e-05\n",
      "                                                            ***** Stochastic Gradient Descent Classifier ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime   1.0050169944763183  0.00038818613692800463\n",
      "1   TestTime  0.03099517822265625  2.7079653668464685e-06\n",
      "2         F1              0.87438    1.19363646056915e-05\n",
      "3   Accuracy   0.8748104523702501   8.893314544353183e-06\n",
      "                                                            ***** Multinomial Naive Bayes ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime   0.07316207885742188   1.158172810565468e-05\n",
      "1   TestTime  0.034777164459228516   6.375162654983092e-06\n",
      "2         F1              0.846124  1.5745333276879163e-05\n",
      "3   Accuracy    0.8464381846427262  1.5347543209876962e-05\n",
      "                                                            ***** Adaboost ****\n",
      "     Measure                Mean              StandError\n",
      "0  TrainTime  25.257717180252076    0.001300239171049622\n",
      "1   TestTime  1.1171419620513916  0.00012338573229640678\n",
      "2         F1  0.8220560000000001   4.224727210128319e-06\n",
      "3   Accuracy  0.8218217541670588   5.041377915575695e-06\n",
      "                                                            ***** Bernullian Naive Bayes ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime   0.09924082756042481  1.3267582994114207e-05\n",
      "1   TestTime  0.042481422424316406   6.345844497396311e-06\n",
      "2         F1    0.8433999999999999    8.51915488766358e-06\n",
      "3   Accuracy    0.8466051346168945   8.565745151292342e-06\n",
      "CPU times: user 37min 26s, sys: 5.04 s, total: 37min 32s\n",
      "Wall time: 37min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learn = classification(train_freq, train.df['Labels'], cv)\n",
    "\n",
    "my_class = ['KNN', 'LSVC', 'Logistic', 'SGD', 'MNB', 'ADAB', 'BNB']\n",
    "\n",
    "for c in my_class:\n",
    "  print(f\"                                                            ***** {decode(c)} ****\")\n",
    "  learn.select_model(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ajMrbVvKU3-"
   },
   "source": [
    "#### ***TF-IDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg3zlcx0Zrwf",
    "outputId": "da29bca0-1128-4280-a117-7eb567452d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 14138)\n",
      "(250000, 1414)\n",
      "CPU times: user 7.35 s, sys: 17 ms, total: 7.37 s\n",
      "Wall time: 7.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "    #-- TF-IDF\n",
    "\n",
    "def dummy(doc):\n",
    "  return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        min_df = 0.0001, # 0.001*100000 = 100, ragionevole\n",
    "        # max_df = 0.7,\n",
    "        ngram_range = (gram_min, gram_max),\n",
    "        binary = False, \n",
    "        # vocabulary = my_dic\n",
    "    )  \n",
    "\n",
    "train_tf_idf = vectorizer.fit_transform(train.df['Clean_Review'])\n",
    "\n",
    "print(train_tf_idf.shape)\n",
    "\n",
    "      #-- FEATURE SELECTION\n",
    "\n",
    "select_percentile = SelectPercentile(chi2, percentile=10)\n",
    "train_tf_idf = select_percentile.fit_transform(train_tf_idf, train.df['Labels'])\n",
    "\n",
    "print(train_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uiwrjLyJ00S",
    "outputId": "7d11eee8-862b-478b-fcdd-5fb2af2f1560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            ***** KNN ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime  0.04699997901916504  1.7428642662618577e-05\n",
      "1   TestTime   349.25233454704284      0.2603049593730844\n",
      "2         F1             0.692836   3.726513008161922e-05\n",
      "3   Accuracy   0.7173839705012711  2.7293960727249858e-05\n",
      "                                                            ***** Linear SVC ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime    2.5988096237182616  0.00026463158242891774\n",
      "1   TestTime  0.030098342895507814  3.2561881359059688e-06\n",
      "2         F1    0.8776119999999998   1.208852679196265e-05\n",
      "3   Accuracy    0.8778765793122693  1.1761169676723211e-05\n",
      "                                                            ***** Logistic ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime    3.3754664421081544     0.00801084806040223\n",
      "1   TestTime  0.030335092544555665   5.927930711854171e-06\n",
      "2         F1              0.875516  1.2709379213793042e-05\n",
      "3   Accuracy    0.8755463237324695  1.2709919639929887e-05\n",
      "                                                            ***** Stochastic Gradient Descent Classifier ****\n",
      "     Measure                  Mean              StandError\n",
      "0  TrainTime    0.4491105556488037  0.00023899173616659424\n",
      "1   TestTime  0.030229663848876952   6.494850144804593e-06\n",
      "2         F1               0.86614  1.3043189793911673e-05\n",
      "3   Accuracy    0.8655416100426271  1.4466704585757553e-05\n",
      "                                                            ***** Multinomial Naive Bayes ****\n",
      "     Measure                 Mean             StandError\n",
      "0  TrainTime  0.06958856582641601   8.81357524808651e-06\n",
      "1   TestTime  0.03277578353881836  3.027162563281756e-06\n",
      "2         F1   0.8480360000000001  2.295467534076651e-05\n",
      "3   Accuracy   0.8477433245530539  2.271257381360717e-05\n",
      "                                                            ***** Adaboost ****\n",
      "     Measure                Mean             StandError\n",
      "0  TrainTime   70.56188073158265  0.0026486292728725384\n",
      "1   TestTime  1.0880373001098633  6.691225969732858e-05\n",
      "2         F1            0.819688  8.927512531495175e-06\n",
      "3   Accuracy  0.8189363279421791   1.00180280255681e-05\n",
      "                                                            ***** Bernullian Naive Bayes ****\n",
      "     Measure                 Mean              StandError\n",
      "0  TrainTime  0.09459972381591797  1.6019590723374364e-05\n",
      "1   TestTime  0.04106717109680176  1.2808954527516646e-05\n",
      "2         F1   0.8471080000000001  1.0476396326982027e-05\n",
      "3   Accuracy   0.8499592927271085  1.0293713103155838e-05\n",
      "CPU times: user 35min 34s, sys: 3.7 s, total: 35min 38s\n",
      "Wall time: 35min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learn = classification(train_tf_idf, train.df['Labels'], cv)\n",
    "\n",
    "my_class = ['KNN', 'LSVC', 'Logistic', 'SGD', 'MNB', 'ADAB', 'BNB']\n",
    "\n",
    "for c in my_class:\n",
    "  print(f\"                                                            ***** {decode(c)} ****\")\n",
    "  learn.select_model(c)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Classificazione(1,1)CVMore.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
